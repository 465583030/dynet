{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression Building\n",
    "\n",
    "(This tutorial is tested on DyNet 2.0.3+ and Python 2.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import dynet_config\n",
    "dynet_config.set(random_seed=0)\n",
    "import dynet as dy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## ==== Create a new computation graph\n",
    "# (it is a singleton, we have one at each stage.\n",
    "# dy.renew_cg() clears the current one and starts a new one)\n",
    "dy.renew_cg();\n",
    "# set random seed to have the same result each time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Expressions\n",
    "Expressions are used as an interface to the various functions that can be used to build DyNet computation graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a scalar expression.\n",
    "value = 5.0\n",
    "x = dy.scalarInput(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a vector expression.\n",
    "dimension = 3\n",
    "v = dy.vecInput(dimension)\n",
    "v.set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a matrix expression from list\n",
    "mat1 = dy.inputTensor([[1,2], [3,4]]) # Row major\n",
    "\n",
    "# or, using numpy array\n",
    "mat2 = dy.inputTensor(np.array([[1,2], [3,4]]))\n",
    "\n",
    "mat3 = dy.inputTensor(np.zeros((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[1.0, 2.0, 3.0]\n",
      "5.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "## ==== We can take the value of an expression. \n",
    "# For complex expressions, this will run forward propagation.\n",
    "print(mat1.value())   \n",
    "print(mat1.npvalue())    # as numpy array\n",
    "print(v.vec_value())     # as vector, if vector\n",
    "print(x.scalar_value())  # as scalar, if scalar\n",
    "print(x.value())         # choose the correct one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Parameters\n",
    "Parameters are things that are optimized. in contrast to a system like Torch where computational modules may have their own parameters, in DyNet parameters are just parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters are things we tune during training.\n",
    "# Usually a matrix or a vector.\n",
    "\n",
    "# First we create a parameter collection and add the parameters to it.\n",
    "m = dy.ParameterCollection() \n",
    "W = m.add_parameters((8,8)) # an 8x8 matrix\n",
    "b = m.add_parameters(8) # an 8x1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noticed that in DyNet 2.0+ and later version, the dy.parameters() is depecated so explicitly adding parameters to the computation graph is no longer necessary. Any used parameter will be added automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are several ways to initial parameters\n",
    "# Specifiying parameter initialization\n",
    "scale = 1\n",
    "mean = 0\n",
    "stddev = 1\n",
    "\n",
    "# Creates 3x5 matrix filled with 0 (or any other float)\n",
    "p = m.add_parameters((3,5), init=0)\n",
    "# Creates 3x5 matrix initialized with U([-scale, scale])\n",
    "p = m.add_parameters((3,5), init='uniform', scale=scale)\n",
    "# Creates 3x5 matrix initialized with N(mean, stddev)\n",
    "p = m.add_parameters((3,5), init='normal', mean=mean, std=stddev)\n",
    "# Creates 5x5 identity matrix\n",
    "p = m.add_parameters((5,5), init='identity')\n",
    "# Creates 3x5 matrix with glorot init\n",
    "p = m.add_parameters((3,5), init='glorot')\n",
    "p = m.add_parameters((3,5)) # By default, the init = 'glorot'\n",
    "# Creates 3x5 matrix with he init\n",
    "p = m.add_parameters((3,5), init='he')\n",
    "# Creates 3x5 matrix from a numpy array (size is inferred)\n",
    "p = m.add_parameters((3,5), np.ones((3,5)))\n",
    "# Creates 3x5 matrix from a numpy array (size is inferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LookupParameters\n",
    "LookupParameters represents a table of parameters. They are used to embed a set of discrete objects (e.g. word embeddings). These are sparsely updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ===== Lookup parameters\n",
    "# Similar to parameters, but are representing a \"lookup table\"\n",
    "# that maps numbers to vectors.\n",
    "# These are used for embedding matrices.\n",
    "# for example, this will have VOCAB_SIZE rows, each of DIM dimensions.\n",
    "VOCAB_SIZE = 100\n",
    "DIM = 10\n",
    "lp = m.add_lookup_parameters((VOCAB_SIZE, DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e45 dim: ((10,), 2)\n",
      "e0_9 dim: ((10,), 10)\n",
      "e5 dim after applying set method ((10,), 1)\n",
      "[0.034212298691272736, -0.10658015310764313, 0.07870276272296906, 0.14834508299827576, 0.05688869580626488, 0.16793382167816162, 0.20482590794563293, 0.21452514827251434, -0.021544886752963066, 0.20709896087646484]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Ceate expressions from lookup parameters.\n",
    "e5  = dy.lookup(lp, 5)   # create an Expression from row 5.\n",
    "e5  = lp[5]              # same\n",
    "e5c = dy.lookup(lp, 5, update=False)  # as before, but don't update when optimizing.\n",
    "\n",
    "e45  = dy.lookup_batch(lp, [4, 5])   # create a batched Expression from rows 4 and 5.\n",
    "e45  = lp.batch([4, 5])\n",
    "print('e45 dim:', e45.dim())\n",
    "\n",
    "e0_9 = dy.lookup_batch(lp, range(10))  # create a batched Expression from rows 0 to 9\n",
    "e0_9 = lp.batch(range(10))\n",
    "print('e0_9 dim:', e0_9.dim())\n",
    "\n",
    "e5.set(10)  # now the e5 expression contains row 10\n",
    "print('e5 dim after applying set method', e5.dim())\n",
    "print(e5.value())\n",
    "\n",
    "# We can check if it is actually containing row 10\n",
    "e10 = lp[10]\n",
    "print(e5.value() == e10.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Similar to Parameters, we have several ways to\n",
    "# initialize LookupParameters.\n",
    "scale = 1\n",
    "mean = 0\n",
    "stddev = 1\n",
    "\n",
    "# Creates 3x5 matrix filled with 0 (or any other float)\n",
    "p = m.add_lookup_parameters((3,5), init=0)\n",
    "# Creates 3x5 matrix initialized with U([-scale, scale])\n",
    "p = m.add_lookup_parameters((3,5), init='uniform', scale=scale)\n",
    "# Creates 3x5 matrix initialized with N(mean, stddev)\n",
    "p = m.add_lookup_parameters((3,5), init='normal', mean=mean, std=stddev)\n",
    "# Creates 5x5 identity matrix\n",
    "p = m.add_lookup_parameters((5,5), init='identity')\n",
    "# Creates 3x5 matrix with glorot init\n",
    "p = m.add_lookup_parameters((3,5), init='glorot')\n",
    "p = m.add_parameters((3,5)) # By default, the init = 'glorot'\n",
    "# Creates 3x5 matrix with he init\n",
    "p = m.add_lookup_parameters((3,5), init='he')\n",
    "# Creates 3x5 matrix from a numpy array (size is inferred)\n",
    "p = m.add_lookup_parameters((3,5), np.ones((3,5)))\n",
    "# Creates 3x5 matrix from a numpy array (size is inferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expression Manipulation\n",
    "DyNet provides tons of operations on Expression. User can manipulate Expressions, build complex Expression easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fist we create some vector Expressions.\n",
    "e1 = dy.vecInput(4)\n",
    "e1.set([1, 2, 3, 4])\n",
    "\n",
    "e2 = dy.vecInput(4)\n",
    "e2.set([5, 6, 7, 8])\n",
    "\n",
    "mat1 = dy.inputTensor(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]))  # A 4x2 matrix\n",
    "mat2 = dy.inputTensor(np.array([[1, 0], [0, 1]]))  # A 4x4 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 8.0, 10.0, 12.0]\n",
      "[4.0, 4.0, 4.0, 4.0]\n",
      "[-1.0, -2.0, -3.0, -4.0]\n",
      "[[ 1.  2.  3.  4.]\n",
      " [ 2.  4.  6.  8.]\n",
      " [ 3.  6.  9. 12.]\n",
      " [ 4.  8. 12. 16.]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "70.0\n",
      "[5.0, 12.0, 21.0, 32.0]\n",
      "[0.20000000298023224, 0.3333333432674408, 0.4285714328289032, 0.5]\n",
      "[[ 2.  3.]\n",
      " [ 5.  6.]\n",
      " [ 8.  9.]\n",
      " [11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Math Operations\n",
    "\n",
    "# Add\n",
    "e = e1 + e2  # Element-wise addition\n",
    "print(e.value())  # Should be [6.0, 8.0, 10.0, 12.0]\n",
    "\n",
    "# Minus\n",
    "e = e2 - e1 # Element-wise minus\n",
    "print(e.value())  # Should be [4.0, 4.0, 4.0, 4.0]\n",
    "# Negative\n",
    "e = -e1  # Should be [-1.0, -2.0, -3.0, -4.0]\n",
    "print(e.value())\n",
    "\n",
    "# Multiply\n",
    "e = e1 * dy.transpose(e1)  #It's Matrix multiplication (like e1.dot(e2) in numpy)\n",
    "print(e.value())\n",
    "\n",
    "mat = mat1 * mat2\n",
    "print(mat.value())\n",
    "\n",
    "# Dot product\n",
    "e = dy.dot_product(e1, e2)  # dot product = sum(component-wise multiply)\n",
    "print(e.value())\n",
    "\n",
    "# Component-wise multiply\n",
    "e = dy.cmult(e1, e2)\n",
    "print(e.value())\n",
    "\n",
    "# Component-wise division\n",
    "e = dy.cdiv(e1, e2)\n",
    "print(e.value())\n",
    "\n",
    "# Column-wise addition\n",
    "# colwise_add(x, y)\n",
    "#  x:  An MxN matrix\n",
    "#  y:  A length M vector\n",
    "mat = dy.colwise_add(mat1, e1)  # column-wise addition\n",
    "print(mat.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 3.]\n",
      " [2. 4.]]\n",
      "e1 dimension: ((4,), 1)\n",
      "e1 transpose dimension ((1, 4), 1)\n"
     ]
    }
   ],
   "source": [
    "# Matrix Shapes\n",
    "\n",
    "# Reshape\n",
    "new_dimension = (2, 2)\n",
    "e = dy.reshape(e1, new_dimension)  # Col major\n",
    "print(e.value())\n",
    "\n",
    "# Transpose\n",
    "e = dy.transpose(e1)\n",
    "print('e1 dimension:', e1.dim())\n",
    "print('e1 transpose dimension', e.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.032058604061603546, 0.08714432269334793, 0.23688283562660217, 0.6439142823219299]\n",
      "[-2.4076058864593506, -1.4076058864593506, -0.4076058864593506, -inf]\n"
     ]
    }
   ],
   "source": [
    "# Per-element unary functions.\n",
    "\n",
    "# exp()\n",
    "e = dy.exp(e1)\n",
    "\n",
    "# sin()\n",
    "e = dy.sin(e1)\n",
    "\n",
    "# cos()\n",
    "e =dy.cos(e1)\n",
    "\n",
    "# tan()\n",
    "e = dy.tan(e1)\n",
    "\n",
    "# asin()\n",
    "e = dy.asin(e1)\n",
    "\n",
    "# acos()\n",
    "e = dy.acos(e1)\n",
    "\n",
    "# atan()\n",
    "e = dy.atan(e1)\n",
    "\n",
    "# sinh()\n",
    "e = dy.sinh(e1)\n",
    "\n",
    "# cosh()\n",
    "e = dy.cosh(e1)\n",
    "\n",
    "# tanh()\n",
    "e = dy.tanh(e1)\n",
    "\n",
    "# log()\n",
    "e = dy.log(e1)\n",
    "\n",
    "# sigmoid()\n",
    "e = dy.logistic(e1)   # Sigmoid(x)\n",
    "\n",
    "# relu()\n",
    "e = dy.rectify(e1)    # Relu (= max(x,0))\n",
    "\n",
    "# softsign()\n",
    "e = dy.softsign(e1)    # x/(1+|x|)\n",
    "\n",
    "# softmax\n",
    "e = dy.softmax(e1)\n",
    "print(e.value())\n",
    "\n",
    "# log_softmax\n",
    "# logsoftmax = logits - log(reduce_sum(exp(logits), dim))\n",
    "# restrict is a set of indices. if not empty, only entries \n",
    "# in restrict are part of softmax computation, others get -inf.\n",
    "e_log_softmax = dy.log_softmax(e1)\n",
    "e_log_softmax = dy.log_softmax(e1, restrict=[0,1,2])\n",
    "print(e_log_softmax.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2 element of vector is 2.0\n",
      "4.0\n",
      "The 2 element of matrix mat1 is [3.0, 4.0]\n",
      "Pick range[k, v) from a vector [2.0, 3.0]\n",
      "[1.0, 2.0, 3.0, 4.0]\n",
      "Pick range[k, v) from a matrix [[3. 4.]\n",
      " [5. 6.]]\n",
      "2.44018983841\n",
      "2.44018959999\n"
     ]
    }
   ],
   "source": [
    "# Picking values from vector expressions\n",
    "\n",
    "k = 1\n",
    "v = 3\n",
    "# Pick one element from a vector or matrix\n",
    "# similar to python's e1[k] for list.\n",
    "# k can be negative, which has exactly the same behavior\n",
    "# as it is in python\n",
    "e = dy.pick(e1, k)\n",
    "print('The {} element of vector is {}'.format(k+1, e.value())) # index starts from 0\n",
    "# which is also equivalent to:\n",
    "e = e1[k]\n",
    "# k can be negative. -1 means the last element\n",
    "e = e1[-1]\n",
    "print(e.value())\n",
    "\n",
    "mat = dy.pick(mat1, k)\n",
    "print('The {} element of matrix mat1 is {}'.format(k+1, mat.value()))\n",
    "# which is equivalent to:\n",
    "mat = mat1[k]\n",
    "\n",
    "# Pick several elements from a vector or matrix\n",
    "# similar to python's e1[k:v] for lists. \n",
    "# e1 is an Expression, k, v are integers.\n",
    "# Important: v should not exceed the e1's dimension.\n",
    "e = dy.pickrange(e1, k, v)\n",
    "print('Pick range[k, v) from a vector', e.value())\n",
    "# which is also equivalent to:\n",
    "e = e1[k:v]\n",
    "e = e1[:v]  # similar to python, you can neglect k\n",
    "e = e1[:]   # or even both k and v\n",
    "print(e.value())\n",
    "# ERROR: Don't try this\n",
    "# e = e1[0:10], the v value should not exceed the dimension.\n",
    "\n",
    "mat = dy.pickrange(mat1, k, v)\n",
    "print('Pick range[k, v) from a matrix', mat.value())\n",
    "\n",
    "# Pick negative log_softmax\n",
    "# which is equivalent to: dy.pick(-dy.log(dy.softmax(e1)), k)\n",
    "e = dy.pickneglogsoftmax(e1, k)\n",
    "e_ = dy.pick(-dy.log(dy.softmax(e1)), k)\n",
    "print(e.value())\n",
    "print(e_.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 5.]\n",
      " [2. 6.]\n",
      " [3. 7.]\n",
      " [4. 8.]]\n",
      "[[1. 2. 5.]\n",
      " [3. 4. 6.]\n",
      " [5. 6. 7.]\n",
      " [7. 8. 8.]]\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[0.0, -1.0, -2.0, -3.0]\n"
     ]
    }
   ],
   "source": [
    "# Expressions concatenation & other useful manipuulations\n",
    "\n",
    "# This performs an elementwise sum over all the expressions included.\n",
    "# All expressions should have the same dimension.\n",
    "e = dy.esum([e1, e2])\n",
    "# which is equivalent to:\n",
    "e_ = e1 + e2\n",
    "assert e.value() == e_.value()\n",
    "\n",
    "# This performs an elementwise average over all the expressions included.\n",
    "# All expressions should have the same dimension.\n",
    "e = dy.average([e1, e2])\n",
    "# which is equivalent to:\n",
    "e_ = (e1 + e2)/2\n",
    "assert e.value() == e_.value()\n",
    "\n",
    "# Concate vectors/matrix column-wise\n",
    "# All expressions should have the same dimension.\n",
    "# e1, e2,.. are column vectors. return a matrix. (sim to np.hstack([e1,e2,...])\n",
    "e = dy.concatenate_cols([e1, e2])\n",
    "print(e.value())\n",
    "\n",
    "mat = dy.concatenate_cols([mat1, e2])\n",
    "print(mat.value())\n",
    "\n",
    "# Concate vectors/matrix\n",
    "# All expressions should have the same dimension.\n",
    "# e1, e2,.. are column vectors. return a matrix. (sim to np.hstack([e1,e2,...])\n",
    "e = dy.concatenate([e1, e2])\n",
    "print(e.value())\n",
    "\n",
    "mat = dy.concatenate([mat2, mat2])\n",
    "print(mat.value())\n",
    "\n",
    "# affine transform\n",
    "e0 = dy.vecInput(2)\n",
    "e0.set([-1, 0])\n",
    "e = dy.affine_transform([e1,mat1,e0])\n",
    "\n",
    "print(e.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DyNet in Neural Networks\n",
    "This part contains Neural Networks related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.979181706905365, 1.8229671716690063, 2.9431982040405273, 4.195302963256836]\n",
      "[2.0, 0.0, 6.0, 8.0]\n",
      "[nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "# Noise and Dropout Expressions\n",
    "\n",
    "# Add a noise to each element from a gausian distribution\n",
    "# with standard-dev = stddev\n",
    "stddev = 0.1\n",
    "e = dy.noise(e1, stddev)\n",
    "print(e.value())\n",
    "\n",
    "# Apply dropout to the input expression\n",
    "# There are two kinds of dropout methods \n",
    "# (http://cs231n.github.io/neural-networks-2)\n",
    "# Dynet implement the Inverted dropout where dropout with prob p \n",
    "# and scaling others by 1/p at training time, and do not need \n",
    "# to do anything at test time. \n",
    "p = 0.5\n",
    "e = dy.dropout(e1, p)    # apply dropout with probability p \n",
    "print(e.value()) # It should be [2.0, 4.0, 6.0, 0.0], the last element is dropped out and the rest are scaled\n",
    "\n",
    "# If we set p=1, everything will be dropped out\n",
    "e = dy.dropout(e1, 1)\n",
    "print(e.value()) # Should be [nan, nan, ...]\n",
    "\n",
    "# If we set p=0, everything will be kept\n",
    "e = dy.dropout(e1, 0)\n",
    "assert e.value() == e1.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0\n",
      "16.0\n",
      "huber distance: 35.8038978577\n",
      "2.77258872986\n",
      "[[5. 5. 5. 5.]]\n",
      "[[0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Loss Functions\n",
    "\n",
    "# DyNet provides several ways to calculate \"distance\"\n",
    "# between two expressions of the same dimension\n",
    "# This is square_distance, defined as\n",
    "# sum(square of(e1-e2)) for all elements\n",
    "# in e1 and e2.\n",
    "# Here e1 is a vector of [1,2,3,4]\n",
    "# And e2 is a vector of [5,6,7,8]\n",
    "# The square distance is sum((5-1)^2 + (6-2)^2+...)\n",
    "e = dy.squared_distance(e1, e2)\n",
    "print(e.value())\n",
    "\n",
    "# This is the l1_distance, defined as \n",
    "# sum (abs(e1-e2)) for all elements in\n",
    "# e1 and e2.\n",
    "e = dy.l1_distance(e1, e2)\n",
    "print(e.value())\n",
    "\n",
    "# This is the huber_distance, definition \n",
    "# found here. (https://en.wikipedia.org/wiki/Huber_loss)\n",
    "# The default threhold (delta) is 1.345.\n",
    "# Here e1 is a vector of [1,2,3,4]\n",
    "# And e2 is a vector of [5,6,7,8]\n",
    "# because for each pair-wised element in\n",
    "# e1 and e2, the abs(e1-e2)=4>delta=1.345,\n",
    "# so the output is sum(delta*(abs(4)-1/2*delta))\n",
    "\n",
    "e = dy.huber_distance(e1, e2, c=1.345)\n",
    "print('huber distance:', e.value()) #TODO: has error here\n",
    "\n",
    "# Binary logistic loss function\n",
    "# This is similar to cross entropy loss function\n",
    "# e1 must be a vector that takes values between 0 and 1\n",
    "# ty must be a vector that takes values between 0 and 1\n",
    "# e = -(ty * log(e1) + (1 - ty) * log(1 - e1))\n",
    "ty = dy.vecInput(4)\n",
    "ty.set([0, 0.5, 0.5, 1])\n",
    "e_scale = ty = dy.vecInput(4)\n",
    "e_scale.set([0.5, 0.5, 0.5, 0.5])\n",
    "e = dy.binary_log_loss(e_scale, ty)\n",
    "print(e.value())\n",
    "# Te binary_log_loss is equivalent to the following:\n",
    "e_equl = -(dy.dot_product(ty, dy.log(e_scale)) + dy.dot_product((dy.inputTensor([1,1,1,1]) - ty), dy.log(dy.inputTensor([1,1,1,1]) - e_scale)))\n",
    "assert e_equl.value() == e.value()\n",
    "\n",
    "# pairwise_rank_loss, a.k.a. Hinge loss\n",
    "# e1 is row vector or scalar\n",
    "# e2 is row vector or scalar\n",
    "# m is number\n",
    "# e = max(0, m - (e1 - e2))\n",
    "e = dy.pairwise_rank_loss(dy.transpose(e1), dy.transpose(e2), m=1.0) # Row vector needed, so we transpose the vector.\n",
    "print(e.value())  # Expect [[5. 5. 5. 5.]]\n",
    "\n",
    "e = dy.pairwise_rank_loss(dy.transpose(e2), dy.transpose(e1), m=1.0) # Row vector needed, so we transpose the vector.\n",
    "print(e.value())  # Expect [[0. 0. 0. 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((4, 4, 3), 1) ((2, 2, 3, 1), 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Bad input dimensions in Conv2D: [{4,4,3} {2,2,3,1} {1,3}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6a50fb4a5b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# b: A vector representing bias. (Ci x 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.conv2d_bias\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_dynet.pyx\u001b[0m in \u001b[0;36m_dynet.conv2d_bias\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Bad input dimensions in Conv2D: [{4,4,3} {2,2,3,1} {1,3}]"
     ]
    }
   ],
   "source": [
    "# Convolutions\n",
    "\n",
    "# DyNet can do convolutions similar to PyTorch.\n",
    "# First we mock an image and a filter\n",
    "# mat is a 3D tensor of dim{4,4,3}\n",
    "# kernel is a 4d Tensor of shape {2,2,3,1}\n",
    "mat = dy.inputTensor(np.array([[[1,2,1], [0,1,2], [0,0,1], [0,1,0]], [[1,0,2], [0,0,0], [1,1,1], [2,2,2]], [[0,1,2], [1,1,0], [0,0,1], [2,2,1]], [[2,2,0], [2,1,2], [2,2,1], [1,1,0]]]))\n",
    "kernel = dy.inputTensor(np.array([[[[1], [0], [2]], [[1], [2], [0]]], [[[0], [1], [0]], [[2], [1], [1]]]]))\n",
    "print(mat.dim(), kernel.dim())\n",
    "# conv2d\n",
    "# This is 2D convolution operator without bias parameters.\n",
    "# dy.conv2d(Expression x, Expression f, vector[unsigned] stride, bool is_valid = True)\n",
    "# x: The input feature maps: (H x W x Ci) x N (ColMaj), 3D tensor with an optional batch dimension\n",
    "# f: 2D convolution filters: H x W x Ci x Co (ColMaj), 4D tensor\n",
    "# stride: the row and column strides in a list\n",
    "# is_valid: padding method. True for 'Valid' and False for 'Same'.\n",
    "#     'Valid': output size shrinks by `filter_size - 1`, and the filters always sweep at valid \n",
    "#              positions inside the input maps. No padding needed.\n",
    "#     'Same': output size is the same with input size. To do so, one needs to pad the input so \n",
    "#             the filter can sweep outside of the input maps.\n",
    "e = dy.conv2d(mat, kernel, stride=[1, 1], is_valid=True)\n",
    "\n",
    "# conv2d_bias\n",
    "# This is 2D convolution operator with bias parameters.\n",
    "# dy.conv2d_bias(Expression x, Expression f, Expression b, vector[unsigned] stride, bool is_valid = True)\n",
    "# b: A vector representing bias. (Ci x 1)\n",
    "b = dy.transpose(dy.inputTensor(np.array([-1, -1, -1])))\n",
    "e = dy.conv2d_bias(mat, kernel, b, stride=[1, 1], is_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutions\n",
    "# e1 \\in R^{d x s} (input)\n",
    "# e2 \\in R^{d x m} (filter)\n",
    "e = dy.conv1d_narrow(e1, e2) # e = e1 *conv e2\n",
    "e = dy.conv1d_wide(e1, e2)   # e = e1 *conv e2\n",
    "e = dy.filter1d_narrow(e1, e2) # e = e1 *filter e2\n",
    "\n",
    "e = dy.kmax_pooling(e1, k) #  kmax-pooling operation (Kalchbrenner et al 2014)\n",
    "e = dy.kmh_ngram(e1, k) # \n",
    "e = dy.fold_rows(e1, nrows=2) #\n",
    "\n",
    "# create parameter collection\n",
    "m = dy.ParameterCollection()\n",
    "\n",
    "# add parameters to parameter collection\n",
    "pW = m.add_parameters((10,30))\n",
    "pB = m.add_parameters(10)\n",
    "lookup = m.add_lookup_parameters((500, 10))\n",
    "print \"added\"\n",
    "\n",
    "# create trainer \n",
    "trainer = dy.SimpleSGDTrainer(m)\n",
    "\n",
    "# Regularization is set via the --dynet-l2 commandline flag.\n",
    "# Learning rate parameters can be passed to the trainer:\n",
    "# alpha = 0.1  # learning rate\n",
    "# trainer = dy.SimpleSGDTrainer(m, e0=alpha)\n",
    "\n",
    "# function for graph creation\n",
    "def create_network_return_loss(inputs, expected_output):\n",
    "    \"\"\"\n",
    "    inputs is a list of numbers\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    W = dy.parameter(pW) # from parameters to expressions\n",
    "    b = dy.parameter(pB)\n",
    "    emb_vectors = [lookup[i] for i in inputs]\n",
    "    net_input = dy.concatenate(emb_vectors)\n",
    "    net_output = dy.softmax( (W*net_input) + b)\n",
    "    loss = -dy.log(dy.pick(net_output, expected_output))\n",
    "    return loss\n",
    "\n",
    "# function for prediction\n",
    "def create_network_return_best(inputs):\n",
    "    \"\"\"\n",
    "    inputs is a list of numbers\n",
    "    \"\"\"\n",
    "    dy.renew_cg()\n",
    "    W = dy.parameter(pW)\n",
    "    b = dy.parameter(pB)\n",
    "    emb_vectors = [lookup[i] for i in inputs]\n",
    "    net_input = dy.concatenate(emb_vectors)\n",
    "    net_output = dy.softmax( (W*net_input) + b)\n",
    "    return np.argmax(net_output.npvalue())\n",
    "\n",
    "\n",
    "# train network\n",
    "for epoch in xrange(5):\n",
    "    for inp,lbl in ( ([1,2,3],1), ([3,2,4],2) ):\n",
    "        print inp, lbl\n",
    "        loss = create_network_return_loss(inp, lbl)\n",
    "        print loss.value() # need to run loss.value() for the forward prop\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "\n",
    "print create_network_return_best([1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Recipe (using classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "# create parameter collection\n",
    "m = dy.ParameterCollection()\n",
    "\n",
    "# create a class encapsulating the network\n",
    "class OurNetwork(object):\n",
    "    # The init method adds parameters to the parameter collection.\n",
    "    def __init__(self, pc):\n",
    "        self.pW = pc.add_parameters((10,30))\n",
    "        self.pB = pc.add_parameters(10)\n",
    "        self.lookup = pc.add_lookup_parameters((500,10))\n",
    "    \n",
    "    # the __call__ method applies the network to an input\n",
    "    def __call__(self, inputs):\n",
    "        W = dy.parameter(self.pW)\n",
    "        b = dy.parameter(self.pB)\n",
    "        lookup = self.lookup\n",
    "        emb_vectors = [lookup[i] for i in inputs]\n",
    "        net_input = dy.concatenate(emb_vectors)\n",
    "        net_output = dy.softmax( (W*net_input) + b)\n",
    "        return net_output\n",
    "    \n",
    "    def create_network_return_loss(self, inputs, expected_output):\n",
    "        dy.renew_cg()\n",
    "        out = self(inputs)\n",
    "        loss = -dy.log(dy.pick(out, expected_output))\n",
    "        return loss\n",
    "       \n",
    "    def create_network_return_best(self, inputs):\n",
    "        dy.renew_cg()\n",
    "        out = self(inputs)\n",
    "        return np.argmax(out.npvalue())\n",
    "        \n",
    "        \n",
    "# create network\n",
    "network = OurNetwork(m)\n",
    "\n",
    "# create trainer \n",
    "trainer = dy.SimpleSGDTrainer(m)\n",
    "   \n",
    "# train network\n",
    "for epoch in xrange(5):\n",
    "    for inp,lbl in ( ([1,2,3],1), ([3,2,4],2) ):\n",
    "        print inp, lbl\n",
    "        loss = network.create_network_return_loss(inp, lbl)\n",
    "        print loss.value() # need to run loss.value() for the forward prop\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "\n",
    "print\n",
    "print network.create_network_return_best([1,2,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### or, alternatively, have the training outside of the network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create network\n",
    "network = OurNetwork(m)\n",
    "\n",
    "# create trainer \n",
    "trainer = dy.SimpleSGDTrainer(m)\n",
    "   \n",
    "# train network\n",
    "for epoch in xrange(5):\n",
    "    for inp,lbl in ( ([1,2,3],1), ([3,2,4],2) ):\n",
    "        print inp, lbl\n",
    "        dy.renew_cg()\n",
    "        out = network(inp)\n",
    "        loss = -dy.log(dy.pick(out, lbl))\n",
    "        print loss.value() # need to run loss.value() for the forward prop\n",
    "        loss.backward()\n",
    "        trainer.update()\n",
    "\n",
    "print\n",
    "print np.argmax(network([1,2,3]).npvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
